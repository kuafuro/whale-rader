import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime, timezone, timedelta
import os  
import gspread 
from google.oauth2.service_account import Credentials
import json

# å–å¾—ç’°å¢ƒè®Šæ•¸
BOT_TOKEN = os.environ.get('TELEGRAM_BOT_TOKEN')
CHAT_ID_WHALE = os.environ.get('TELEGRAM_CHAT_ID_WHALE') 

GCP_CREDENTIALS = os.environ.get('GCP_CREDENTIALS')
SPREADSHEET_ID = os.environ.get('SPREADSHEET_ID')
worksheet = None

# ğŸŒŸ Form 144 é–€æª»é€šå¸¸è¼ƒé«˜ï¼Œæˆ‘å€‘è¨­å®šè¿½è¹¤ã€Œæº–å‚™æ‹‹å”®è¶…é 100 è¬ç¾é‡‘ã€çš„å¤§æ¡ˆå­
MIN_PROPOSED_SALE = 1000000  
STRICT_WATCHLIST = True # ä¿æŒèˆ‡ Form 4 ç›¸åŒçš„åš´æ ¼æŠŠé—œ

# ğŸŒŸ æ–°å¢ï¼šå°ˆå±¬æ–¼ Form 144 é›·é”çš„æš«å­˜è¨˜æ†¶é«”
processed_links = set()
CACHE_FILE = 'processed_links_form144.txt'

if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, 'r') as f:
        processed_links.update(f.read().splitlines())

# é€£æ¥ Google Sheets ä¸¦æŠ“å–æ­·å²ç¶²å€ (å»é‡é˜²ç·š)
if GCP_CREDENTIALS and SPREADSHEET_ID:
    try:
        creds_dict = json.loads(GCP_CREDENTIALS)
        scopes = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
        creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        gc = gspread.authorize(creds)
        sh = gc.open_by_key(SPREADSHEET_ID)
        worksheet = sh.sheet1 
        print("âœ… Google Sheets é€£ç·šæˆåŠŸï¼(Form 144 é›·é”)")
        
        try:
            # æŠ“å–ç¬¬ 7 æ¬„ (Gæ¬„) çš„æœ€è¿‘ 200 ç­†æ­·å²ç¶²å€
            sheet_links = worksheet.col_values(7)[-200:]
            processed_links.update(sheet_links)
        except Exception as e:
            print(f"âš ï¸ è®€å– Google Sheets æ­·å²ç´€éŒ„å¤±æ•—: {e}")
            
    except Exception as e:
        print(f"âŒ Google Sheets åˆå§‹åŒ–å¤±æ•—: {e}")

def get_sp500_tickers():
    try:
        url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        tickers = set()
        for row in soup.find('table', {'id': 'constituents'}).find_all('tr')[1:]:
            ticker = row.find_all('td')[0].text.strip()
            tickers.add(ticker); tickers.add(ticker.replace('.', '-'))
        return tickers
    except:
        return set()

SP500_TICKERS = get_sp500_tickers()

# ğŸŒŸ ä¿®æ­£ï¼šå…¨é¢æ”¹ç”¨ POST ç™¼é€ï¼Œé¿å… Telegram URL é•·åº¦é™åˆ¶
def send_whale_telegram(message):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {'chat_id': CHAT_ID_WHALE, 'text': message, 'parse_mode': 'HTML'}
    try:
        requests.post(url, data=payload)
    except Exception as e:
        print(f"Telegram æ¨æ’­å¤±æ•—: {e}")

# ğŸŒŸ ä¿®æ­£ï¼šæ›ä¸ŠçœŸå¯¦ä¿¡ç®±ï¼Œèº²é¿ SEC ç‹™æ“Š
headers = {'User-Agent': 'Form144RadarBot/2.0 (mingcheng@kuafuorhk.com)'}
url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&type=144&owner=include&count=40&output=atom'

now_utc = datetime.now(timezone.utc)
time_limit = now_utc - timedelta(minutes=15)

try:
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'xml')
    entries = soup.find_all('entry')

    for entry in entries:
        link = entry.link['href']
        updated_str = entry.updated.text
        
        # ğŸŒŸ æ–°å¢ï¼šå¦‚æœé€™å€‹é€£çµå·²ç¶“æ¨æ’­éï¼Œç›´æ¥è·³éï¼
        if link in processed_links:
            continue
        
        try:
            if datetime.fromisoformat(updated_str.replace('Z', '+00:00')).astimezone(timezone.utc) < time_limit: 
                break 
        except Exception as e:
            print(f"æ™‚é–“è§£æå¤±æ•—ï¼Œè·³éæ­¤ç­†ç´€éŒ„ ({updated_str}): {e}")
            continue # ğŸŒŸ ä¿®æ­£ï¼šæ™‚é–“å£æ‰ç›´æ¥è·³éï¼Œä¸å¾€ä¸‹åŸ·è¡Œ

        txt_link = link.replace('-index.htm', '.txt')
        
        # ğŸŒŸ ä¿®æ­£ï¼šåŠ ä¸Š SEC é€Ÿç‡é™åˆ¶ä¿è­·
        time.sleep(0.15)
        
        txt_response = requests.get(txt_link, headers=headers)
        
        if txt_response.status_code == 200:
            xml_soup = BeautifulSoup(txt_response.content, 'xml')
            try:
                issuer_name_tag = xml_soup.find('issuerName')
                issuer_name = issuer_name_tag.text if issuer_name_tag else "æœªçŸ¥å…¬å¸"
                
                seller_tag = xml_soup.find('nameOfPersonForWhoseAccountTheSecuritiesAreToBeSold')
                seller_name = seller_tag.text if seller_tag else "æœªçŸ¥é«˜ç®¡/å¤§è‚¡æ±"
                
                ticker_tag = xml_soup.find('issuerTradingSymbol')
                ticker = ticker_tag.text if ticker_tag else "N/A"
                
                # ğŸŒŸ ä¿®æ­£ï¼šS&P 500 é˜²ç·šé‚è¼¯å‡ç´š
                if STRICT_WATCHLIST:
                    if not SP500_TICKERS or (ticker not in SP500_TICKERS):
                        continue
                        
                market_value_tag = xml_soup.find('aggregateMarketValue')
                market_value_str = market_value_tag.text if market_value_tag else "0"
                
                try:
                    market_value = float(market_value_str)
                except:
                    market_value = 0
                    
                if market_value >= MIN_PROPOSED_SALE:
                    msg = f"ğŸš¨ <b>ã€æ°´æ™¶çƒé è­¦ï¼šForm 144 æº–å‚™æ‹‹å”®ï¼ã€‘</b>\n"
                    msg += f"ğŸ¢ å…¬å¸: <b>{issuer_name}</b> (${ticker})\n"
                    msg += f"ğŸ‘¤ æ‹‹å”®æ–¹: <b>{seller_name}</b>\n"
                    msg += f"ğŸ’€ é è¨ˆå€’è²¨è¦æ¨¡: <b>${market_value:,.0f}</b> ç¾é‡‘\n"
                    msg += f"âš ï¸ <i>(æ³¨æ„ï¼šæ­¤ç‚ºæ‹‹å”®æ„å‘ï¼Œè‚¡ç¥¨å¯èƒ½å³å°‡æµå…¥å¸‚å ´)</i>\n"
                    msg += f"ğŸ”— <a href='{link}'>æŸ¥çœ‹ SEC åŸæ–‡</a>"
                    
                    send_whale_telegram(msg)
                    
                    # ğŸŒŸ æ–°å¢ï¼šå°é½Šå¯«å…¥ Google Sheets ç´€éŒ„
                    if worksheet:
                        try:
                            time_str = datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')
                            # æ ¼å¼å°é½Š [æ™‚é–“, ä»£ç¢¼, å…¬å¸åç¨±, é¡å‹, æ‹‹å”®æ–¹, ç¸½é¡, ç¶²å€(ç¬¬7æ¬„)]
                            row_data = [time_str, ticker, issuer_name, "ğŸ”´ æº–å‚™æ‹‹å”® (Form 144)", seller_name, market_value, link]
                            worksheet.append_row(row_data)
                        except Exception as e:
                            print(f"å¯«å…¥ Google è¡¨æ ¼å¤±æ•—: {e}")

                    # ğŸŒŸ æ–°å¢ï¼šæˆåŠŸè™•ç†å¾Œï¼ŒæŠŠç¶²å€å¯«å…¥è¨˜æ†¶é«”èˆ‡æœ¬åœ°æš«å­˜æª”
                    processed_links.add(link)
                    with open(CACHE_FILE, 'a') as f:
                        f.write(link + '\n')

                    time.sleep(1.5)
                    
            except Exception as e:
                print(f"è§£æ Form 144 å…§éƒ¨è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

except Exception as e:
    print(f"Form 144 é›·é”åŸ·è¡Œç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
